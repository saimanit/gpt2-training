# -*- coding: utf-8 -*-
"""gptlike_trainer_for_resume.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zwcGRpFn4ccJ3FFG_oNM0wwJlF2UJ_LT
"""

!pip install PyPDF2
!pip install datasets
!pip install transformers

import torch
from PyPDF2 import PdfReader
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, EvalPrediction

# Step 1: Load PDF and extract text
pdf_path = "Sai_Mani_Teja.pdf"
reader = PdfReader(pdf_path)
resume_text = ""
for page in reader.pages:
    resume_text += page.extract_text() + "\n"

print("Extracted Resume Text:", resume_text)

# Step 2: Initialize Tokenizer & Model (GPT-2)
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Ensure the tokenizer has a padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Step 3: Tokenize Resume Text
encoded_resume = tokenizer(
    resume_text,
    truncation=True,
    padding="max_length",
    max_length=512,
    return_tensors="pt",
    add_special_tokens=True
)

# Step 4: Define Custom Dataset Class
class ResumeDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = item['input_ids'].clone()
        return item

    def __len__(self):
        return len(self.encodings.input_ids)

resume_dataset = ResumeDataset(encoded_resume)

# Step 5: Define Metric Calculation Function
def compute_metrics(eval_pred: EvalPrediction):
    logits, labels = eval_pred
    loss_fct = torch.nn.CrossEntropyLoss()
    loss = loss_fct(torch.tensor(logits[:, :-1, :]).reshape(-1, model.config.vocab_size),
                    torch.tensor(labels[:, 1:]).reshape(-1))
    perplexity = torch.exp(loss)
    return {"loss": loss.item(), "perplexity": perplexity.item()}

# Step 6: Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=2,
    num_train_epochs=5,
    logging_dir='./logs',
    logging_steps=10
)

# Step 7: Initialize Trainer (as before)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=resume_dataset,
    eval_dataset=resume_dataset,
    compute_metrics=compute_metrics
)

# Step 8: Train the Model
train_result = trainer.train()
print("Model trained successfully!")

# Optional: Print training metrics
print("Training Loss:", train_result.training_loss)  # This is the average training loss

# Step 9: Evaluate the model after training
eval_results = trainer.evaluate()
print("Evaluation Loss:", eval_results["eval_loss"])
print("Evaluation Perplexity:", eval_results["eval_perplexity"])

# Step 10: Define Prompt for Text Generation
prompt = (
    "As a software engineer, I have expertise in cloud computing, data engineering, "
    "and software development. My experience includes designing and deploying enterprise-scale "
    "applications using AWS, Java, Python, and SQL. "
    "I have successfully built RESTful APIs with Spring Boot and optimized database queries for performance. "
    "Additionally, I have led projects integrating machine learning models into production environments."
)

# Step 11: Tokenize Prompt & Move to Device
input_prompt = tokenizer(prompt, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_prompt = {key: val.to(device) for key, val in input_prompt.items()}  # Move tensors to same device
model.to(device)

# Step 12: Generate Text Using Trained Model
generated_ids = model.generate(
    input_ids=input_prompt['input_ids'],
    attention_mask=input_prompt['attention_mask'],
    max_new_tokens=150,  # Control length
    temperature=0.1,  # Extremely low temp for deterministic, factual responses
    top_k=20,  # Lower randomness by selecting only the top tokens
    top_p=0.80,  # Further limit probability mass for predictable output
    repetition_penalty=4.0,  # Stronger penalty to stop hallucinations & repetition
    num_beams=5,  # Higher beam search for structured, refined results
    early_stopping=True,  # Ensures the output doesn‚Äôt go off-track
    no_repeat_ngram_size=3  # Prevents repeating 3-word phrases
)

# Step 13: Decode & Print Generated Text
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print("\nüìù Generated Resume-Based Text:\n", generated_text)





